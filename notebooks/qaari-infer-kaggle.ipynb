{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14529806,"datasetId":9280170,"databundleVersionId":15358385},{"sourceType":"datasetVersion","sourceId":14529810,"datasetId":9280173,"databundleVersionId":15358389}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --upgrade \\\n    huggingface_hub \\\n    \"transformers>=4.44.0,<5.0.0\" \\\n    \"accelerate>=0.30.0\" \\\n    \"peft>=0.11.0\" \\\n    qwen_vl_utils\n!pip install bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:16:08.983502Z","iopub.execute_input":"2026-02-15T16:16:08.983709Z","iopub.status.idle":"2026-02-15T16:16:33.022797Z","shell.execute_reply.started":"2026-02-15T16:16:08.983689Z","shell.execute_reply":"2026-02-15T16:16:33.021953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:24:06.501983Z","iopub.execute_input":"2026-02-15T16:24:06.502331Z","iopub.status.idle":"2026-02-15T16:24:07.333242Z","shell.execute_reply.started":"2026-02-15T16:24:06.502300Z","shell.execute_reply":"2026-02-15T16:24:07.332451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"code = r'''\nimport os\nimport time\nimport torch\nimport multiprocessing\nimport gc\nimport json\nimport fcntl\nimport tempfile\nimport shutil\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional, Set\nfrom PIL import Image\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom huggingface_hub import HfApi, snapshot_download\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    HF_TOKEN = UserSecretsClient().get_secret(\"HF_WRITE\")\n    print(\"‚úÖ HF token loaded\")\nexcept:\n    HF_TOKEN = None\n    print(\"‚ö†Ô∏è HF token not found - sync disabled\")\n\n# Configuration\nDATASET_DIRS = [\n    \"/kaggle/input/pats-a01-data\",\n]\nHF_REPO_ID = \"Mohamed109/ocr-results\"\nMODEL_PATH = \"NAMAA-Space/Qari-OCR-0.1-VL-2B-Instruct\"\nBASE_OUTPUT_DIR = \"/kaggle/working/results\"\nFAILED_IMAGES_LOG = \"/kaggle/working/failed_images.json\"\nSYNC_STATE_FILE = \"/kaggle/working/sync_state.json\"\nSYNC_INTERVAL = 50\nUSE_BOTH_GPUS = True\nINSTANCES_PER_GPU = 1\nMAX_NEW_TOKENS = 2000\nLOG_INTERVAL = 10\n\n# Performance tuning\nAGGRESSIVE_CLEANUP = False\nCLEANUP_INTERVAL = 50\nMAX_RETRIES_PER_WORKER = 2\nENABLE_CROSS_WORKER_RETRY = True\n\n# Rate limit / retry config\nMAX_API_RETRIES = 5\nINITIAL_BACKOFF_SECONDS = 10\nMAX_BACKOFF_SECONDS = 300\n\n\ndef retry_with_backoff(func, *args, max_retries=MAX_API_RETRIES, initial_backoff=INITIAL_BACKOFF_SECONDS, **kwargs):\n    \"\"\"Generic retry wrapper with exponential backoff for HF API calls.\"\"\"\n    last_exception = None\n    for attempt in range(max_retries):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n            error_str = str(e).lower()\n            # Only retry on rate limits (429) or server errors (5xx)\n            is_rate_limit = \"429\" in str(e) or \"rate limit\" in error_str\n            is_server_error = any(code in str(e) for code in [\"500\", \"502\", \"503\", \"504\"])\n            \n            if not (is_rate_limit or is_server_error):\n                raise  # Don't retry client errors\n            \n            backoff = min(initial_backoff * (2 ** attempt), MAX_BACKOFF_SECONDS)\n            # If the error message contains a suggested wait time, use it\n            if is_rate_limit:\n                import re\n                wait_match = re.search(r'waiting\\s+(\\d+\\.?\\d*)\\s*s', error_str)\n                if wait_match:\n                    suggested = float(wait_match.group(1))\n                    backoff = max(backoff, suggested + 5)  # Add 5s buffer\n            \n            print(f\"  ‚ö†Ô∏è API error (attempt {attempt + 1}/{max_retries}): {type(e).__name__}\")\n            print(f\"  ‚è≥ Retrying in {backoff:.0f}s...\", flush=True)\n            time.sleep(backoff)\n    \n    raise last_exception\n\n\nclass SyncStateTracker:\n    \"\"\"Thread-safe tracker with file locking for tracking synced files\"\"\"\n    def __init__(self, state_file: str):\n        self.state_file = state_file\n        self.lock_file = state_file + \".lock\"\n    \n    def _load_state(self) -> Set[str]:\n        if os.path.exists(self.state_file):\n            try:\n                with open(self.state_file, 'r') as f:\n                    data = json.load(f)\n                    return set(data.get(\"synced_files\", []))\n            except:\n                pass\n        return set()\n    \n    def _save_state(self, synced_files: Set[str]):\n        temp_file = self.state_file + \".tmp\"\n        with open(temp_file, 'w') as f:\n            json.dump({\"synced_files\": list(synced_files)}, f)\n        os.replace(temp_file, self.state_file)\n    \n    def mark_synced(self, file_paths: List[str]):\n        if not os.path.exists(self.lock_file):\n            open(self.lock_file, 'w').close()\n        \n        with open(self.lock_file, 'r') as lock_fd:\n            fcntl.flock(lock_fd.fileno(), fcntl.LOCK_EX)\n            try:\n                current_state = self._load_state()\n                current_state.update(file_paths)\n                self._save_state(current_state)\n            finally:\n                fcntl.flock(lock_fd.fileno(), fcntl.LOCK_UN)\n    \n    def get_synced_files(self) -> Set[str]:\n        return self._load_state()\n    \n    def get_unsynced_files(self, base_dir: str) -> List[str]:\n        synced_files = self._load_state()\n        all_files = []\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.endswith('.txt'):\n                    full_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(full_path, base_dir)\n                    if rel_path not in synced_files:\n                        all_files.append(full_path)\n        return all_files\n\n\nclass QaariOCR:\n    def __init__(self, model_name: str, max_tokens: int, device: str, use_flash_attn: bool=False):\n        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n        import torch\n        \n        self.device = device\n        \n        if use_flash_attn:\n            print(f\"Inferencing with flash attention on {device}...\")\n            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch.bfloat16,\n                attn_implementation=\"flash_attention_2\",\n            ).to(device)\n        else:\n            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch.float16,\n            ).to(device)\n        \n        self.processor = AutoProcessor.from_pretrained(model_name)\n        self.max_tokens = max_tokens\n        self.model.eval()\n\n    def __call__(self, _: str, image: Image, worker_id: int, reduced_quality: bool = False) -> str:\n        from qwen_vl_utils import process_vision_info\n        \n        src = f\"qaari_image_{worker_id}_{os.getpid()}.png\"\n        \n        if reduced_quality:\n            max_dimension = 1024\n            if max(image.size) > max_dimension:\n                ratio = max_dimension / max(image.size)\n                new_size = tuple(int(dim * ratio) for dim in image.size)\n                image = image.resize(new_size, Image.Resampling.LANCZOS)\n\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        image.save(src)\n        \n        try:\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\", \"image\": f\"file://{src}\"},\n                        {\"type\": \"text\", \"text\": \"Transcribe the Arabic text in this image exactly as it appears. Output the text in a natural reading order. Do not hallucinate or add any commentary.\"},\n                    ],\n                }\n            ]\n            text = self.processor.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            image_inputs, video_inputs = process_vision_info(messages)\n            inputs = self.processor(\n                text=[text],\n                images=image_inputs,\n                videos=video_inputs,\n                padding=True,\n                return_tensors=\"pt\",\n            )\n            inputs = inputs.to(self.device)\n            \n            max_tokens = self.max_tokens // 2 if reduced_quality else self.max_tokens\n            \n            with torch.no_grad():\n                generated_ids = self.model.generate(**inputs, max_new_tokens=max_tokens, use_cache=True)\n            \n            generated_ids_trimmed = [\n                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n            ]\n            output_text = self.processor.batch_decode(\n                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )[0]\n            \n            del inputs, generated_ids, generated_ids_trimmed, image_inputs, video_inputs\n            \n            return output_text\n        finally:\n            if os.path.exists(src):\n                os.remove(src)\n\n\nclass FailedImagesTracker:\n    def __init__(self, log_file: str):\n        self.log_file = log_file\n        self.local_failures = []\n    \n    def add_failed(self, img_path: str, out_path: str, worker_id: int, error: str):\n        self.local_failures.append({\n            \"img_path\": img_path,\n            \"out_path\": out_path,\n            \"worker_id\": worker_id,\n            \"error\": str(error)[:100]\n        })\n    \n    def flush_to_disk(self):\n        if not self.local_failures:\n            return\n        \n        existing = {}\n        if os.path.exists(self.log_file):\n            try:\n                with open(self.log_file, 'r') as f:\n                    existing = json.load(f)\n            except:\n                pass\n        \n        for failure in self.local_failures:\n            key = failure[\"img_path\"]\n            if key not in existing:\n                existing[key] = {\n                    \"img_path\": failure[\"img_path\"],\n                    \"out_path\": failure[\"out_path\"],\n                    \"attempts\": []\n                }\n            existing[key][\"attempts\"].append({\n                \"worker_id\": failure[\"worker_id\"],\n                \"error\": failure[\"error\"]\n            })\n        \n        with open(self.log_file, 'w') as f:\n            json.dump(existing, f, indent=2)\n        \n        self.local_failures.clear()\n    \n    def load_failures(self) -> dict:\n        if os.path.exists(self.log_file):\n            try:\n                with open(self.log_file, 'r') as f:\n                    return json.load(f)\n            except:\n                return {}\n        return {}\n    \n    def get_retry_candidates(self, max_attempts: int = 4) -> List[Tuple[str, str]]:\n        failures = self.load_failures()\n        retry_list = []\n        for key, data in failures.items():\n            if len(data[\"attempts\"]) < max_attempts and not os.path.exists(data[\"out_path\"]):\n                retry_list.append((data[\"img_path\"], data[\"out_path\"]))\n        return retry_list\n    \n    def get_final_failures(self) -> List[dict]:\n        failures = self.load_failures()\n        final_failures = []\n        for key, data in failures.items():\n            if not os.path.exists(data[\"out_path\"]) and len(data[\"attempts\"]) >= 4:\n                final_failures.append(data)\n        return final_failures\n\n\nclass OCRWorker:\n    def __init__(self, worker_id: int, gpu_id: int, should_sync: bool = False):\n        self.worker_id = worker_id\n        self.gpu_id = gpu_id\n        self.device = f\"cuda:{gpu_id}\"\n        self.should_sync = should_sync and HF_TOKEN\n        self.api = HfApi(token=HF_TOKEN) if self.should_sync else None\n        self.sync_tracker = SyncStateTracker(SYNC_STATE_FILE) if self.should_sync else None\n        self.ocr_engine = None\n        self.failed_tracker = FailedImagesTracker(FAILED_IMAGES_LOG)\n        self.images_since_cleanup = 0\n        self.newly_created_files = []\n    \n    def load_model(self) -> bool:\n        print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] Loading model...\", flush=True)\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            self.ocr_engine = QaariOCR(\n                model_name=MODEL_PATH,\n                max_tokens=MAX_NEW_TOKENS,\n                device=self.device,\n                use_flash_attn=False\n            )\n            \n            if torch.cuda.is_available():\n                allocated = torch.cuda.memory_allocated(self.gpu_id) / 1024**3\n                print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] Memory: {allocated:.2f}GB allocated\", flush=True)\n            \n            print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] Ready!\", flush=True)\n            return True\n        except Exception as e:\n            print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] FAILED: {e}\", flush=True)\n            return False\n    \n    def cleanup_memory(self, force: bool = False):\n        if force or AGGRESSIVE_CLEANUP:\n            if torch.cuda.is_available():\n                gc.collect()\n                torch.cuda.empty_cache()\n    \n    def periodic_cleanup(self):\n        self.images_since_cleanup += 1\n        if self.images_since_cleanup >= CLEANUP_INTERVAL:\n            self.cleanup_memory(force=True)\n            self.images_since_cleanup = 0\n    \n    def process_image(self, img_path: str, out_path: str, retry_count: int = 0) -> bool:\n        try:\n            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n            \n            image = Image.open(img_path).convert(\"RGB\")\n            reduced_quality = retry_count > 0\n            output = self.ocr_engine(\"\", image, self.worker_id, reduced_quality=reduced_quality)\n            \n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(output)\n            \n            self.newly_created_files.append(out_path)\n            self.periodic_cleanup()\n            return True\n            \n        except torch.cuda.OutOfMemoryError as e:\n            error_msg = f\"OOM (attempt {retry_count + 1})\"\n            print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] {error_msg}: {os.path.basename(img_path)}\", flush=True)\n            \n            self.failed_tracker.add_failed(img_path, out_path, self.worker_id, error_msg)\n            self.cleanup_memory(force=True)\n            \n            if retry_count < MAX_RETRIES_PER_WORKER:\n                time.sleep(2)\n                return self.process_image(img_path, out_path, retry_count + 1)\n            else:\n                return False\n                \n        except Exception as e:\n            error_msg = f\"{type(e).__name__}\"\n            self.failed_tracker.add_failed(img_path, out_path, self.worker_id, error_msg)\n            self.cleanup_memory(force=True)\n            return False\n    \n    def sync_incremental(self, processed_count: int) -> bool:\n        \"\"\"Batch upload NEW files in a single commit with retry logic.\"\"\"\n        if not self.should_sync or not self.newly_created_files:\n            return True\n        \n        num_new = len(self.newly_created_files)\n        print(f\"\\n[Worker {self.worker_id} | GPU {self.gpu_id}] [SYNC] Uploading {num_new} files in batch...\", flush=True)\n        \n        temp_dir = tempfile.mkdtemp()\n        \n        try:\n            # Copy files to temp directory maintaining structure\n            for file_path in self.newly_created_files:\n                rel_path = os.path.relpath(file_path, BASE_OUTPUT_DIR)\n                temp_file_path = os.path.join(temp_dir, rel_path)\n                os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)\n                shutil.copy2(file_path, temp_file_path)\n            \n            # Upload with retry + exponential backoff\n            def _do_upload():\n                self.api.upload_folder(\n                    folder_path=temp_dir,\n                    repo_id=HF_REPO_ID,\n                    repo_type=\"dataset\",\n                    path_in_repo=\"results\",\n                    commit_message=f\"Worker {self.worker_id}: Add {num_new} files (checkpoint at {processed_count})\"\n                )\n            \n            retry_with_backoff(_do_upload)\n            \n            # Mark files as synced\n            rel_paths = [os.path.relpath(f, BASE_OUTPUT_DIR) for f in self.newly_created_files]\n            self.sync_tracker.mark_synced(rel_paths)\n            self.newly_created_files.clear()\n            \n            print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] [SYNC] ‚úì {num_new} files uploaded\", flush=True)\n            return True\n            \n        except Exception as e:\n            print(f\"[Worker {self.worker_id} | GPU {self.gpu_id}] [SYNC] Failed after retries: {e}\", flush=True)\n            # Keep files in list to retry next time\n            return False\n        finally:\n            shutil.rmtree(temp_dir, ignore_errors=True)\n    \n    def run(self, tasks: List[Tuple[str, str]]):\n        if not self.load_model():\n            return\n        \n        total = len(tasks)\n        processed = skipped = failed = new_since_sync = 0\n        start_time = time.time()\n        \n        for idx, (img, out) in enumerate(tasks, 1):\n            if os.path.exists(out):\n                skipped += 1\n                continue\n            \n            if self.process_image(img, out):\n                processed += 1\n                new_since_sync += 1\n            else:\n                failed += 1\n            \n            if self.should_sync and new_since_sync >= SYNC_INTERVAL:\n                self.sync_incremental(processed)\n                new_since_sync = 0\n            \n            if idx % LOG_INTERVAL == 0:\n                elapsed = time.time() - start_time\n                speed = processed / elapsed if elapsed > 0 else 0\n                eta = (total - idx) / speed if speed > 0 else 0\n                \n                mem_info = \"\"\n                if torch.cuda.is_available() and idx % (LOG_INTERVAL * 5) == 0:\n                    allocated = torch.cuda.memory_allocated(self.gpu_id) / 1024**3\n                    mem_info = f\" | Mem:{allocated:.2f}GB\"\n                \n                print(\n                    f\"[Worker {self.worker_id} | GPU {self.gpu_id}] {idx}/{total} | \"\n                    f\"Done:{processed} Skip:{skipped} Fail:{failed} | \"\n                    f\"{speed:.2f} img/s | ETA:{eta/60:.1f}m{mem_info}\",\n                    flush=True\n                )\n        \n        self.failed_tracker.flush_to_disk()\n        \n        if self.should_sync and self.newly_created_files:\n            self.sync_incremental(processed)\n        \n        print(f\"\\n[Worker {self.worker_id} | GPU {self.gpu_id}] DONE! Processed:{processed} Skipped:{skipped} Failed:{failed}\")\n\n\ndef worker_process(worker_id: int, gpu_id: int, tasks: List[Tuple[str, str]], should_sync: bool):\n    worker = OCRWorker(worker_id, gpu_id, should_sync)\n    worker.run(tasks)\n\n\ndef fetch_existing_remote_files() -> Set[str]:\n    \"\"\"\n    Instead of downloading all files (which triggers 429 on thousands of small files),\n    use the HF API to LIST files in the repo. This is a single API call, not thousands.\n    Returns a set of relative paths (e.g., \"pats-a01-data/A01-Naskh/Naskh_0001.txt\").\n    \"\"\"\n    if not HF_TOKEN:\n        return set()\n    \n    print(f\"‚òÅÔ∏è Listing existing files in {HF_REPO_ID}...\", flush=True)\n    \n    try:\n        api = HfApi(token=HF_TOKEN)\n        \n        # list_repo_tree or list_repo_files ‚Äî single paginated API call, no downloads\n        def _list_files():\n            return api.list_repo_files(\n                repo_id=HF_REPO_ID,\n                repo_type=\"dataset\",\n            )\n        \n        all_files = retry_with_backoff(_list_files)\n        \n        # Filter to only results/*.txt and strip the \"results/\" prefix\n        remote_results = set()\n        prefix = \"results/\"\n        for f in all_files:\n            if f.startswith(prefix) and f.endswith(\".txt\"):\n                rel_path = f[len(prefix):]  # e.g. \"pats-a01-data/A01-Naskh/Naskh_0001.txt\"\n                remote_results.add(rel_path)\n        \n        print(f\"‚úÖ Found {len(remote_results)} existing result files remotely\", flush=True)\n        return remote_results\n        \n    except Exception as e:\n        print(f\"‚ÑπÔ∏è Could not list remote files: {e}\", flush=True)\n        return set()\n\n\ndef scan_datasets() -> List[Tuple[str, str]]:\n    print(\"\\n\" + \"=\"*70)\n    print(\"SCANNING\")\n    print(\"=\"*70)\n    tasks = []\n    extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff', '.webp')\n    for ds_dir in DATASET_DIRS:\n        if not os.path.exists(ds_dir):\n            print(f\"‚ö†Ô∏è Not found: {ds_dir}\")\n            continue\n        print(f\"üìÅ {ds_dir}\")\n        count = 0\n        for root, _, files in os.walk(ds_dir):\n            for file in files:\n                if any(file.lower().endswith(ext) for ext in extensions):\n                    inp = os.path.join(root, file)\n                    rel = os.path.relpath(inp, \"/kaggle/input\")\n                    out = os.path.join(BASE_OUTPUT_DIR, os.path.splitext(rel)[0] + \".txt\")\n                    tasks.append((inp, out))\n                    count += 1\n        print(f\"   {count} images\")\n    print(f\"\\n‚úÖ Total: {len(tasks)}\")\n    return tasks\n\n\ndef retry_failed_images():\n    if not ENABLE_CROSS_WORKER_RETRY:\n        return\n    \n    tracker = FailedImagesTracker(FAILED_IMAGES_LOG)\n    retry_candidates = tracker.get_retry_candidates(max_attempts=4)\n    \n    if not retry_candidates:\n        print(\"\\n‚úÖ No images need retry\")\n        return\n    \n    print(f\"\\n\" + \"=\"*70)\n    print(f\"RETRY PHASE - {len(retry_candidates)} images\")\n    print(\"=\"*70)\n    \n    num_gpus = 2 if USE_BOTH_GPUS else 1\n    chunk_size = len(retry_candidates) // num_gpus\n    \n    processes = []\n    for gpu_id in range(num_gpus):\n        start_idx = gpu_id * chunk_size\n        end_idx = start_idx + chunk_size if gpu_id < num_gpus - 1 else len(retry_candidates)\n        gpu_tasks = retry_candidates[start_idx:end_idx]\n        \n        if gpu_tasks:\n            print(f\"GPU {gpu_id}: {len(gpu_tasks)} retry tasks\")\n            p = multiprocessing.Process(\n                target=worker_process,\n                args=(100 + gpu_id, gpu_id, gpu_tasks, True)\n            )\n            processes.append(p)\n    \n    for p in processes:\n        p.start()\n    for p in processes:\n        p.join()\n    \n    print(\"=\"*70)\n\n\ndef print_final_report():\n    tracker = FailedImagesTracker(FAILED_IMAGES_LOG)\n    final_failures = tracker.get_final_failures()\n    \n    if final_failures:\n        print(f\"\\n\" + \"=\"*70)\n        print(f\"‚ö†Ô∏è FINAL REPORT - {len(final_failures)} images could not be processed\")\n        print(\"=\"*70)\n        for failure in final_failures[:10]:\n            print(f\"  - {os.path.basename(failure['img_path'])} ({len(failure['attempts'])} attempts)\")\n        if len(final_failures) > 10:\n            print(f\"  ... and {len(final_failures) - 10} more\")\n        print(f\"\\nFull log: {FAILED_IMAGES_LOG}\")\n        print(\"=\"*70)\n    else:\n        print(f\"\\n‚úÖ All images processed successfully!\")\n\n\ndef main():\n    print(\"\\n\" + \"=\"*70)\n    print(\"KAGGLE OCR PIPELINE\")\n    print(\"=\"*70)\n    print(f\"Model: {MODEL_PATH}\")\n    print(f\"Output: {BASE_OUTPUT_DIR}\")\n    print(f\"Sync: {'ON (incremental)' if HF_TOKEN else 'OFF'}\")\n    print(f\"GPUs: {'Both' if USE_BOTH_GPUS else 'Single'}\")\n    print(f\"Instances per GPU: {INSTANCES_PER_GPU}\")\n    print(\"=\"*70)\n    \n    # ---- KEY FIX: Use API listing instead of snapshot_download ----\n    # This avoids thousands of individual HTTP GETs that trigger 429.\n    remote_files = fetch_existing_remote_files()\n    \n    # Also update the sync state tracker so we don't re-upload these\n    if remote_files:\n        sync_tracker = SyncStateTracker(SYNC_STATE_FILE)\n        sync_tracker.mark_synced(list(remote_files))\n    \n    all_tasks = scan_datasets()\n    \n    if not all_tasks:\n        print(\"\\n‚ùå No images found!\")\n        return\n    \n    # Filter: skip tasks where output exists locally OR remotely\n    print(\"\\nüîç Filtering...\")\n    pending = []\n    done_local = 0\n    done_remote = 0\n    for img_path, out_path in all_tasks:\n        if os.path.exists(out_path):\n            done_local += 1\n            continue\n        # Check if already processed remotely (without downloading)\n        rel_path = os.path.relpath(out_path, BASE_OUTPUT_DIR)\n        if rel_path in remote_files:\n            done_remote += 1\n            # Create a placeholder locally so workers skip it\n            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n            with open(out_path, 'w') as f:\n                f.write(\"\")  # Empty placeholder ‚Äî will be overwritten if needed\n            # Actually we should NOT create empty placeholders as that corrupts results.\n            # Instead, just mark as done and skip.\n            continue\n        pending.append((img_path, out_path))\n    \n    print(f\"   ‚úì Done locally: {done_local}\")\n    print(f\"   ‚úì Done remotely: {done_remote}\")\n    print(f\"   ‚è≠Ô∏è Remaining: {len(pending)}\")\n    \n    if not pending:\n        print(\"\\nüéâ All done!\")\n        print_final_report()\n        return\n    \n    # PHASE 1: Initial processing\n    num_gpus = 2 if USE_BOTH_GPUS else 1\n    num_workers = num_gpus * INSTANCES_PER_GPU\n    \n    chunk_size = len(pending) // num_workers\n    remainder = len(pending) % num_workers\n    \n    worker_tasks = []\n    start_idx = 0\n    for i in range(num_workers):\n        size = chunk_size + (1 if i < remainder else 0)\n        end_idx = start_idx + size\n        worker_tasks.append(pending[start_idx:end_idx])\n        start_idx = end_idx\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"PHASE 1: INITIAL PROCESSING\")\n    print(\"=\"*70)\n    \n    worker_id = 0\n    for gpu_id in range(num_gpus):\n        for instance in range(INSTANCES_PER_GPU):\n            print(f\"Worker {worker_id} (GPU {gpu_id}): {len(worker_tasks[worker_id])} tasks\")\n            worker_id += 1\n    print(\"=\"*70)\n    \n    processes = []\n    worker_id = 0\n    \n    for gpu_id in range(num_gpus):\n        for instance in range(INSTANCES_PER_GPU):\n            p = multiprocessing.Process(\n                target=worker_process,\n                args=(worker_id, gpu_id, worker_tasks[worker_id], True)\n            )\n            processes.append(p)\n            worker_id += 1\n    \n    for p in processes:\n        p.start()\n    for p in processes:\n        p.join()\n    \n    # PHASE 2: Retry failed images\n    if ENABLE_CROSS_WORKER_RETRY:\n        retry_failed_images()\n    \n    # FINAL REPORT\n    print(\"\\n\" + \"=\"*70)\n    print(\"üéâ PIPELINE COMPLETE!\")\n    print(\"=\"*70)\n    print_final_report()\n\n\nif __name__ == \"__main__\":\n    multiprocessing.set_start_method('spawn', force=True)\n    main()\n'''\n\n# Write to file\nwith open('ocr_pipeline.py', 'w') as f:\n    f.write(code)\n\nprint(\"‚úÖ Code written to ocr_pipeline.py\")\n\n# ============================================================================\n# STEP 2: Execute the pipeline\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXECUTING PIPELINE\")\nprint(\"=\"*70 + \"\\n\")\n\n# exec(open('ocr_pipeline.py').read())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:16:48.556887Z","iopub.execute_input":"2026-02-15T16:16:48.557582Z","iopub.status.idle":"2026-02-15T16:16:48.574437Z","shell.execute_reply.started":"2026-02-15T16:16:48.557554Z","shell.execute_reply":"2026-02-15T16:16:48.573787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python ocr_pipeline.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T16:16:49.319670Z","iopub.execute_input":"2026-02-15T16:16:49.319949Z","execution_failed":"2026-02-15T16:19:19.835Z"}},"outputs":[],"execution_count":null}]}